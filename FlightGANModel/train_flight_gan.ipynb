{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(gen, disc, mapping_network, inputs, latent_dim, criterion, device, features=14):\n",
    "  \"\"\" Trains the discrinator of the GAN model.\n",
    "\n",
    "      Args:\n",
    "          gen: The generator of the model currently being trained\n",
    "          disc: The discriminator of the model currently being trained\n",
    "          map: The mapping network of the model currently being trained\n",
    "          inputs: The real images found in our data set\n",
    "          latent_dim: The latent dimension of our noise vector\n",
    "          criterion: The loss function used for our model\n",
    "          device: The device used for cuda\n",
    "    \"\"\"\n",
    "\n",
    "  batchSize = inputs.size(0)\n",
    "\n",
    "  # Get real images from data set\n",
    "  real_images = inputs#.to(device)\n",
    "  realLabels = torch.ones(batchSize, device = device, dtype=torch.float16)\n",
    "\n",
    "  # Get w from mapping network \n",
    "  w = None\n",
    "  # Determines whether we should implement style mixing or not\n",
    "  if torch.rand(()).item() < 0:\n",
    "    cross = int(torch.rand(()).item() * features)\n",
    "    # Randomly samples 2 latent vectors\n",
    "    z1, z2 = torch.randn(batchSize, 512).to(device), torch.randn(batchSize, 512).to(device)\n",
    "    w1 = mapping_network(z1)[None, :, :].expand(cross, -1, -1)\n",
    "    w2 = mapping_network(z2)[None, :, :].expand(features - cross, -1, -1)\n",
    "\n",
    "    # Mixes the styles generated \n",
    "    w = torch.cat((w1, w2), dim=0)\n",
    "  else:\n",
    "    z = torch.randn(batchSize, 512).to(device)\n",
    "\n",
    "    # Does not mix styles\n",
    "    w = mapping_network(z)\n",
    "    w = w[None, :, :].expand(features, -1, -1)\n",
    "    \n",
    "\n",
    "  w = torch.transpose(w, 0, 1)\n",
    "\n",
    "  # Create fake images with generator\n",
    "  fake_images = gen.forward(w, None)\n",
    "\n",
    "  fakeLabels = torch.zeros(batchSize, device = device)\n",
    "\n",
    "  # Get discriminator loss on real images\n",
    "  discPredReal = disc.forward(real_images)\n",
    "  realLoss = criterion(discPredReal.squeeze(1), realLabels) \n",
    "\n",
    "  # Get discriminator loss on fake images\n",
    "  discPredFake = disc.forward(fake_images)\n",
    "  fakeLoss = criterion(discPredFake.squeeze(1), fakeLabels) \n",
    "\n",
    "  # Take average of losses\n",
    "  discLoss = 0.5 * (realLoss + fakeLoss) \n",
    "\n",
    "  return realLabels, fakeLabels, discLoss, discPredReal\n",
    "\n",
    "def train_generator(gen, disc, mapping_network, inputs, criterion, device, features=14):\n",
    "  \"\"\" Trains the generator of the GAN model.\n",
    "\n",
    "    Args:\n",
    "        gen: The generator of the model currently being trained\n",
    "        disc: The discriminator of the model currently being trained\n",
    "        map: The mapping network of the model currently being trained\n",
    "        inputs: The fake images generated by our generator in the discriminator training\n",
    "        realLabels: A tensor of ones to label the images as real\n",
    "        criterion: The loss function used for our model\n",
    "        device: The device used for cuda\n",
    "    \"\"\" \n",
    "\n",
    "  batchSize = inputs.size(0)\n",
    "  realLabels = torch.ones(batchSize, device = device)\n",
    "\n",
    "  # Get w from mapping network\n",
    "  w = None\n",
    "\n",
    "  # Determines whether we should implement style mixing or not\n",
    "  if torch.rand(()).item() < 0:\n",
    "    cross = int(torch.rand(()).item() * features)\n",
    "    # Randomly samples 2 latent vectors\n",
    "    z1, z2 = torch.randn(batchSize, 512).to(device), torch.randn(batchSize, 512).to(device)\n",
    "    w1 = mapping_network(z1)[None, :, :].expand(cross, -1, -1)\n",
    "    w2 = mapping_network(z2)[None, :, :].expand(features - cross, -1, -1)\n",
    "\n",
    "    # Mixes the styles generated \n",
    "    w = torch.cat((w1, w2), dim=0)\n",
    "  else:\n",
    "    z = torch.randn(batchSize, 512).to(device)\n",
    "\n",
    "    # Does not mix styles\n",
    "    w = mapping_network(z)\n",
    "    w = w[None, :, :].expand(features, -1, -1)\n",
    "\n",
    "\n",
    "  w = torch.transpose(w, 0, 1)\n",
    "\n",
    "  # Create fake images with generator\n",
    "  fake_images = gen.forward(w, None)\n",
    "\n",
    "  # Get discriminator loss on fake images with real labels instead of fake labels\n",
    "  discPredFake = disc.forward(fake_images)\n",
    "\n",
    "  genLoss = criterion(discPredFake.squeeze(1), realLabels)\n",
    "\n",
    "\n",
    "  return genLoss, discPredFake, fake_images\n",
    "\n",
    "  \n",
    "def train_network(generator, discriminator, mapping, interval, device, trainLoader, num_epochs, lrGAN = 1e-5, lrMap = 1e-5, latent_dim = 512, cuda=True):\n",
    "    \"\"\" Trains the model entirely (both the generator and discriminator) and stores results.\n",
    "\n",
    "    Args:\n",
    "        gen: The generator of the model currently being trained\n",
    "        disc: The discriminator of the model currently being trained\n",
    "        map: The mapping network of the model currently being trained\n",
    "        latent_dim: The latent dimension of our noise vector\n",
    "        device: The device being used for cuda\n",
    "        trainLoader: The training data for our model to access during training\n",
    "        num_epochs: The number of epochs to run our model for\n",
    "        lrGAN: The learning rate of our generator and discriminator\n",
    "        lrMap: The learning rate of our mapping network\n",
    "        interval: The iteration interval at which we print our model's loss values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set manual seed for reproducible results\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    # Criterion and optimizers\n",
    "    criterion = nn.BCEWithLogitsLoss() # necessary if we use our loss function and not theirs\n",
    "    gen_optimizer =  optim.Adam(generator.parameters(), lr= lrGAN, weight_decay=1e-3)\n",
    "    disc_optimizer =  optim.Adam(discriminator.parameters(), lr=lrGAN, weight_decay=1e-3)\n",
    "    map_optimizer = optim.Adam(mapping.parameters(), lr = lrMap, weight_decay = 1e-3)\n",
    "\n",
    "    # Training metrics\n",
    "    train_gen_loss = []\n",
    "    train_disc_loss = []\n",
    "    train_disc_real_acc = []\n",
    "    train_disc_fake_acc = []\n",
    "    newImgs = []\n",
    "\n",
    "    inputNoise = nn.Parameter(torch.randn([7, 2]), requires_grad=False)\n",
    "    inputW = mapping(torch.randn(batchSize, 512).to(device))\n",
    "    inputW = inputW[None, :, :].expand(features, -1, -1)\n",
    "    inputW = torch.transpose(inputW, 0, 1)\n",
    "\n",
    "    ########## SENDING TO CUDA ############\n",
    "    if cuda:\n",
    "        generator = generator.to('cuda:0')\n",
    "        discriminator = discriminator.to('cuda:0')\n",
    "        mapping = mapping.to('cuda:0')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        mapping.train()\n",
    "        iteration = 0\n",
    "    \n",
    "        for imgs, _ in trainLoader:\n",
    "\n",
    "            if cuda:\n",
    "               imgs = imgs.cuda()\n",
    "\n",
    "            # Train Discriminator\n",
    "            disc_optimizer.zero_grad()\n",
    "            realLabels, fakeLabels, discLoss, discPredReal = train_discriminator(generator, discriminator, mapping, imgs, latent_dim, criterion, device)\n",
    "\n",
    "            discLoss.backward(retain_graph=True)            \n",
    "            disc_optimizer.step()\n",
    "        \n",
    "            # Train Generator and Mapping Network\n",
    "            gen_optimizer.zero_grad()\n",
    "            map_optimizer.zero_grad()\n",
    "            genLoss, discPredFake, fake_images = train_generator(generator, discriminator, mapping, imgs, criterion, device)\n",
    "            genLoss.backward(retain_graph=True)\n",
    "            gen_optimizer.step()\n",
    "            map_optimizer.step()\n",
    "\n",
    "            # Calculate accuracy of model\n",
    "            predReal = torch.where(discPredReal.detach() > 0.0, 1.0, 0.0)\n",
    "            predFake = torch.where(discPredFake.detach() > 0.0, 1.0, 0.0)\n",
    "            accReal = (predReal == realLabels).float().mean() * 100\n",
    "            accFake = (predFake == fakeLabels).float().mean() * 100\n",
    "\n",
    "            # Store discriminator loss and accuracies, and generator loss\n",
    "            train_disc_loss.append(discLoss.item())\n",
    "            # Maybe add  torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
    "            train_gen_loss.append(genLoss.item())  \n",
    "            # Maybe add  torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)         \n",
    "            train_disc_real_acc.append(accReal.item())\n",
    "            train_disc_fake_acc.append(accFake.item())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            # If at iteration interval, print current results of model\n",
    "            if iteration % interval == 0:\n",
    "              print(\"Epoch: %03d/%03d | Batch %03d/%03d | Gen/Disc Loss: %.4f/%.4f\"\n",
    "                        % (epoch + 1, num_epochs, iteration, len(trainLoader),\n",
    "                            genLoss.item(), discLoss.item()))\n",
    "              \n",
    "        # Store images created by generator every 100 epoch\n",
    "        if epoch % 10 == 0:\n",
    "          # Use constant input w vector and noise to see how our generator improves on same data\n",
    "          tempImg = generator.forward(inputW, inputNoise)\n",
    "          imshow(tempImg, title=None)\n",
    "          \n",
    "        print(\"Time elapsed: %.2f min\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    return train_gen_loss, train_disc_loss, train_disc_real_acc, train_disc_fake_acc, newImgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44634398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2102]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.1839]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.1544]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.1275]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.0992]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.0704]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.0421]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.0156]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.9879]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.9601]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.9334]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.9056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.8773]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.8498]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.8245]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.7974]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.7709]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.7421]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.7137]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.6866]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.6611]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.6345]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.6068]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.5819]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.5568]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.5292]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.5045]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.4789]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.4519]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.4251]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.4015]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.3764]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.3506]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.3263]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.3025]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.2754]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.2534]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.2298]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.2054]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.1807]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.1553]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.1325]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.1098]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.0854]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.0622]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.0417]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-2.0182]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.9943]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.9720]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.9510]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.9282]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.9055]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.8843]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.8637]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.8402]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.8226]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.8010]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.7818]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.7607]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.7401]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.7229]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.7022]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.6816]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.6635]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.6448]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.6242]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.6059]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.5883]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.5698]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.5520]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.5342]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.5176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4999]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4844]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4677]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4499]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4348]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4180]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4023]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.3854]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.3718]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.3558]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.3413]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.3265]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.3135]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2994]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2853]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2712]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2425]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2284]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2164]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2032]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1909]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1788]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1647]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1554]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1416]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1303]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Epoch: 001/030 | Batch 100/18922 | Gen/Disc Loss: 1.4005/0.8444\n",
      "tensor([[-1.1081]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.0980]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.0862]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.0748]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\stylegan2\\train.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m disc_net \u001b[39m=\u001b[39m Discriminator()\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_network(gen_net\u001b[39m.\u001b[39;49mSynthesisNet, disc_net, gen_net\u001b[39m.\u001b[39;49mMappingNet, \u001b[39m100\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m'\u001b[39;49m, train_loader, \u001b[39m30\u001b[39;49m)\n",
      "\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\stylegan2\\train.ipynb Cell 20\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(generator, discriminator, mapping, interval, device, trainLoader, num_epochs, lrGAN, lrMap, latent_dim, cuda)\u001b[0m\n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=199'>200</a>\u001b[0m \u001b[39m# Train Discriminator\u001b[39;00m\n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m disc_optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m realLabels, fakeLabels, discLoss, discPredReal \u001b[39m=\u001b[39m train_discriminator(generator, discriminator, mapping, imgs, latent_dim, criterion, device)\n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m discLoss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)            \n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m disc_optimizer\u001b[39m.\u001b[39mstep()\n",
      "\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\stylegan2\\train.ipynb Cell 20\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[1;34m(gen, disc, mapping_network, inputs, latent_dim, criterion, device, features)\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m realLoss \u001b[39m=\u001b[39m criterion(discPredReal\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), realLabels) \u001b[39m#they use different loss function (wasserstein)\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Get discriminator loss on fake images\u001b[39;00m\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m discPredFake \u001b[39m=\u001b[39m disc\u001b[39m.\u001b[39;49mforward(fake_images)\u001b[39m#.view(-1)\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m fakeLoss \u001b[39m=\u001b[39m criterion(discPredFake\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), fakeLabels) \u001b[39m#they use different loss function (wasserstein)\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Different Loss\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# realLoss = F.relu(1 - discPredReal).mean()\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# fakeLoss = F.relu(1 + discPredFake).mean()\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# discLoss = realLoss + fakeLoss\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# Take average of losses\u001b[39;00m\n",
      "\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\stylegan2\\train.ipynb Cell 20\u001b[0m in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip1(x)\n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n",
      "\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdown(x)\n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m x \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39madd_(x) \u001b[39m*\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip2(x)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n",
      "\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n",
      "\u001b[0;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "\u001b[0;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\stylegan2\\train.ipynb Cell 20\u001b[0m in \u001b[0;36mDownsampleBlock.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(x)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Blur input with kernel and reshape input\u001b[39;00m\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mconv2d(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(b, c, h, w)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/stylegan2/train.ipynb#X26sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Scale down the input and return\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from importnb import Notebook\n",
    "\n",
    "with Notebook():\n",
    "    from flight_gan_model import Generator, Discriminator\n",
    "\n",
    "gen_net = Generator()\n",
    "disc_net = Discriminator()\n",
    "\n",
    "train_network(gen_net.SynthesisNet, disc_net, gen_net.MappingNet, 100, 'cuda:0', train_loader, 30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
